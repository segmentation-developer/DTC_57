
Namespace(D_lr=0.0001, base_lr=0.01, batch_size=4, beta=0.3, consistency=1.0, consistency_rampup=40.0, consistency_type='kl', consistency_weight=0.1, deterministic=1, ema_decay=0.99, exp='LA/SSL_DTC_DenseModi_conandUnet', gamma=0.5, gpu='4,5,6,7', labeled_bs=2, labelnum=16, max_iterations=6000, root_path='/data/sohui/LA_dataset/2018LA_Seg_TrainingSet', seed=1340, with_cons='without_cons')
  0%|                                         | 0/751 [00:00<?, ?it/s]
total 80 samples
  0%|                                         | 0/751 [00:00<?, ?it/s]/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/functional.py:3060: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn("Default upsampling behavior when mode={} is changed "
  0%|                                         | 0/751 [00:11<?, ?it/s]
Traceback (most recent call last):
  File "/home/sohui/code/DTC/code/SSL_train_la_dtc_VNetandUnet.py", line 234, in <module>
    loss.backward()
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 736.00 MiB (GPU 0; 10.92 GiB total capacity; 9.62 GiB already allocated; 129.44 MiB free; 10.07 GiB reserved in total by PyTorch)