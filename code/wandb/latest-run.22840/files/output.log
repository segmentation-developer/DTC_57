
Namespace(D_lr=0.0001, base_lr=0.004, batch_size=4, beta=0.3, consistency=1.0, consistency_rampup=40.0, consistency_type='kl', consistency_weight=0.1, deterministic=1, ema_decay=0.99, exp='LA/SSL_DTC_VNet_convNext', gamma=0.5, gpu='1', labeled_bs=2, labelnum=16, max_iterations=6000, root_path='/data/sohui/LA_dataset/2018LA_Seg_TrainingSet', seed=1337, with_cons='without_cons')
total 80 samples
8 itertations per epoch
  0%|                                         | 0/751 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/sohui/code/DTC/code/SSL_train_la_dtc_VNet.py", line 199, in <module>
    outputs_tanh, outputs = model(volume_batch)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sohui/code/DTC/code/networks/vnet_convNext_sdf.py", line 253, in forward
    features = self.encoder(input)
  File "/home/sohui/code/DTC/code/networks/vnet_convNext_sdf.py", line 195, in encoder
    x2 = self.block_two(x1_dw)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sohui/code/DTC/code/networks/vnet_convNext_sdf.py", line 32, in forward
    x = self.conv(x)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 572, in forward
    return F.conv3d(input, self.weight, self.bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 10.92 GiB total capacity; 10.19 GiB already allocated; 33.44 MiB free; 10.21 GiB reserved in total by PyTorch)