
Namespace(D_lr=0.0001, base_lr=0.01, batch_size=4, beta=0.3, consistency=1.0, consistency_rampup=40.0, consistency_type='kl', consistency_weight=0.1, deterministic=1, ema_decay=0.99, exp='LA/SSL_DTC_DenseModi_decPointwise_conandUnet', gamma=0.5, gpu='5,6', labeled_bs=2, labelnum=16, max_iterations=6000, root_path='/data/sohui/LA_dataset/2018LA_Seg_TrainingSet', seed=1340, with_cons='without_cons')
  0%|                                         | 0/751 [00:00<?, ?it/s]
total 80 samples
  0%|                                         | 0/751 [00:00<?, ?it/s]/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/functional.py:3060: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn("Default upsampling behavior when mode={} is changed "
  0%|                                         | 0/751 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/home/sohui/code/DTC/code/SSL_train_la_dtc_VNetandUnet.py", line 208, in <module>
    outputs2 = model2(volume_batch)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 161, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 171, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/_utils.py", line 428, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sohui/code/DTC/code/networks/unet3D.py", line 147, in forward
    up1 = self.up_concat1(conv1, up2)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sohui/code/DTC/code/networks/unet3D.py", line 73, in forward
    return self.conv(torch.cat([outputs1, outputs2], 1))
RuntimeError: CUDA out of memory. Tried to allocate 736.00 MiB (GPU 0; 10.92 GiB total capacity; 9.50 GiB already allocated; 681.44 MiB free; 9.55 GiB reserved in total by PyTorch)