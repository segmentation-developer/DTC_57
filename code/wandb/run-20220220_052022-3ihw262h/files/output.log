
Namespace(D_lr=0.0001, base_lr=0.01, batch_size=4, beta=0.3, consistency=1.0, consistency_rampup=40.0, consistency_type='kl', consistency_weight=0.1, deterministic=1, ema_decay=0.99, exp='LA/SSL_DTC_VNet_attentionandUnet', gamma=0.5, gpu='4,5,6,7', labeled_bs=2, labelnum=16, max_iterations=6000, root_path='/data/sohui/LA_dataset/2018LA_Seg_TrainingSet', seed=1340, with_cons='without_cons')
  0%|                                         | 0/751 [00:00<?, ?it/s]
total 80 samples
  0%|                                         | 0/751 [00:00<?, ?it/s]/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/functional.py:3060: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn("Default upsampling behavior when mode={} is changed "
/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
  0%|                                         | 0/751 [00:03<?, ?it/s]
Traceback (most recent call last):
  File "/home/sohui/code/DTC/code/SSL_train_la_dtc_VNetandUnet.py", line 205, in <module>
    outputs1 = model1(volume_batch)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sohui/code/DTC/code/networks/vnet_attention.py", line 305, in forward
    out = self.decoder(features)
  File "/home/sohui/code/DTC/code/networks/vnet_attention.py", line 279, in decoder
    g_conv1, att1 = self.attentionblock1(x1, x7_up) #4,16,112,,112,80
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sohui/code/DTC/code/networks/vnet_attention.py", line 338, in forward
    gate_1, attention_1 = self.gate_block_1(input, gating_signal)
  File "/home/sohui/anaconda3/envs/unetr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sohui/code/DTC/code/utils/grid_attention_layer.py", line 81, in forward
    output = self.operation_function(x, g)
  File "/home/sohui/code/DTC/code/utils/grid_attention_layer.py", line 104, in _concatenation
    y = sigm_psi_f.expand_as(x) * x
RuntimeError: CUDA out of memory. Tried to allocate 980.00 MiB (GPU 0; 10.92 GiB total capacity; 9.99 GiB already allocated; 215.44 MiB free; 10.03 GiB reserved in total by PyTorch)